{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB 3 Bayesian Regression: A Comparsion\n",
    "\n",
    "Introduction\n",
    "Reference: Yves Hilpisch: \"Python for Finance\" Chapter 11. 2015.\n",
    "\n",
    "This laborary is to demonstrate the difference between classical regression and Baynesian regression.  \n",
    "\n",
    "With PyMC3 the Python ecosystem provides a powerful and performant library to technically implement Bayesian statistics. PyMC3 is already a powerful library at the time of this writing. However, it is still in its early stages, so you should expect further enhancements, changes to the API, etc. \n",
    "\n",
    "As a benchmark, consider first an ordinary least-squares regression given the noisy data, the result of the “standard” regression approach is fixed values for the parameters of the regression line. Whereas, the Bayesian regression approach, we assume that the parameters are distributed in a certain way. \n",
    "\n",
    "For example, consider the equation describing the regression line ŷ ( x ) = a + bx , it is assumed the following priors : \n",
    "\n",
    "a is normally distributed with mean 0 and a standard deviation of 20.\n",
    "b is normally distributed with mean 0 and a standard deviation of 20.\n",
    "For the likelihood , we assume a normal distribution with mean of ŷ ( x ) and a uniformly distributed standard deviation between 0 and 10. A major element of Bayesian regression is (Markov Chain) Monte Carlo (MCMC) sampling.  \n",
    "In principle, this is the same as drawing balls multiple times from boxes, as in the previous simple example just in a more systematic, automated way. \n",
    "\n",
    "For the technical sampling, there are three different functions to call: \"find_MAP\" finds the starting point for the sampling algorithm by deriving the local maximum a posteriori point .\n",
    "\n",
    "\"NUTS\" implements the so-called “efficient No-U-Turn Sampler with dual averaging” (NUTS) algorithm for MCMC sampling given the assumed priors.\n",
    "\n",
    "\"Sample\" draws a number of samples given the starting value from find_MAP and the optimal step size from the NUTS algorithm.\n",
    "\n",
    "All this is to be wrapped into a PyMC3 Model object and executed within an API: \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Library, PyMC3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.simplefilter('ignore') \n",
    "import pymc3 as pm \n",
    "import numpy as np \n",
    "np.random.seed(1000) \n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import glm as glm_sm\n",
    "import arviz as az\n",
    "az.style.use('arviz-darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baynesian Idea Exposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below it is a binormal random trial with different probability of success: 0.25, .5, 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = [1, 2, 4]  # Number of trials\n",
    "p_params = [0.25, 0.5, 0.75]  # Probability of success\n",
    "\n",
    "x = np.arange(0, max(n_params)+1)\n",
    "f,ax = plt.subplots(len(n_params), len(p_params), sharex=True, sharey=True,\n",
    "                    figsize=(8, 7), constrained_layout=True)\n",
    "\n",
    "for i in range(len(n_params)):\n",
    "    for j in range(len(p_params)):\n",
    "        n = n_params[i]\n",
    "        p = p_params[j]\n",
    "\n",
    "        y = stats.binom(n=n, p=p).pmf(x)\n",
    "\n",
    "        ax[i,j].vlines(x, 0, y, colors='C0', lw=5)\n",
    "        ax[i,j].set_ylim(0, 1)\n",
    "        ax[i,j].plot(0, 0, label=\"N = {:3.2f}\\nθ = {:3.2f}\".format(n,p), alpha=0)\n",
    "        ax[i,j].legend()\n",
    "\n",
    "        ax[2,1].set_xlabel('y')\n",
    "        ax[1,0].set_ylabel('p(y|θ,N)')\n",
    "        ax[0,0].set_xticks(x)\n",
    "\n",
    "plt.savefig('B11197_01_03.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Trial the Coin to detemine the unknown fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create a list of the number of coin tosses (\"Bernoulli trials\")\n",
    "    number_of_trials = [0, 2, 10, 20, 50, 500]\n",
    "\n",
    "    # Conduct 500 coin tosses and output into a list of 0s and 1s\n",
    "    # where 0 represents a tail and 1 represents a head\n",
    "    data = stats.bernoulli.rvs(0.5, size=number_of_trials[-1])\n",
    "    \n",
    "    # Discretise the x-axis into 100 separate plotting points\n",
    "    x = np.linspace(0, 1, 100)\n",
    "    \n",
    "    # Loops over the number_of_trials list to continually add\n",
    "    # more coin toss data. For each new set of data, we update\n",
    "    # our (current) prior belief to be a new posterior. This is\n",
    "    # carried out using what is known as the Beta-Binomial model.\n",
    "    # For the time being, we won't worry about this too much. It \n",
    "    # will be the subject of a later article!\n",
    "    for i, N in enumerate(number_of_trials):\n",
    "        # Accumulate the total number of heads for this \n",
    "        # particular Bayesian update\n",
    "        heads = data[:N].sum()\n",
    "\n",
    "        # Create an axes subplot for each update \n",
    "        ax = plt.subplot(len(number_of_trials) / 2, 2, i + 1)\n",
    "        ax.set_title(\"%s trials, %s heads\" % (N, heads))\n",
    "\n",
    "        # Add labels to both axes and hide labels on y-axis\n",
    "        plt.xlabel(\"$P(H)$, Probability of Heads\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        if i == 0:\n",
    "            plt.ylim([0.0, 2.0])\n",
    "        plt.setp(ax.get_yticklabels(), visible=False)\n",
    "                \n",
    "        # Create and plot a  Beta distribution to represent the \n",
    "        # posterior belief in fairness of the coin.\n",
    "        y = stats.beta.pdf(x, 1 + heads, 1 + N - heads)\n",
    "        plt.plot(x, y, label=\"observe %d tosses,\\n %d heads\" % (N, heads))\n",
    "        plt.fill_between(x, 0, y, color=\"#aaaadd\", alpha=0.5)\n",
    "\n",
    "    # Expand plot to cover full width/height and show it\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate the Beta Distribution based on different hyperparameters of alpha and beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [0.5, 1, 2, 3]\n",
    "x = np.linspace(0, 1, 100)\n",
    "f, ax = plt.subplots(len(params), len(params), sharex=True, sharey=True,\n",
    "                     figsize=(8, 7), constrained_layout=True)\n",
    "for i in range(4):\n",
    "    for j in range(4):\n",
    "        a = params[i]\n",
    "        b = params[j]\n",
    "        y = stats.beta(a, b).pdf(x)\n",
    "        ax[i,j].plot(x, y)\n",
    "        ax[i,j].plot(0, 0, label=\"α = {:2.1f}\\nβ = {:2.1f}\".format(a, b), alpha=0)\n",
    "        ax[i,j].legend()\n",
    "ax[1,0].set_yticks([])\n",
    "ax[1,0].set_xticks([0, 0.5, 1])\n",
    "f.text(0.5, 0.05, 'θ', ha='center')\n",
    "f.text(0.07, 0.5, 'p(θ)', va='center', rotation=0)\n",
    "plt.savefig('B11197_01_04.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from different belief of theta (based on different hyperparameters of alpha and beta to generate different prior different distribution (colour: orange, green and purple) of theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "n_trials = [0, 1, 2, 3, 4, 8, 16, 32, 50, 150]\n",
    "data = [0, 1, 1, 1, 1, 4, 6, 9, 13, 48]\n",
    "theta_real = 0.35\n",
    "\n",
    "beta_params = [(1, 1), (20, 20), (1, 4)]\n",
    "dist = stats.beta\n",
    "x = np.linspace(0, 1, 200)\n",
    "\n",
    "for idx, N in enumerate(n_trials):\n",
    "    if idx == 0:\n",
    "        plt.subplot(4, 3, 2)\n",
    "        plt.xlabel('θ')\n",
    "    else:\n",
    "        plt.subplot(4, 3, idx+3)\n",
    "        plt.xticks([])\n",
    "    y = data[idx]\n",
    "    for (a_prior, b_prior) in beta_params:\n",
    "        p_theta_given_y = dist.pdf(x, a_prior + y, b_prior + N - y)\n",
    "        plt.fill_between(x, 0, p_theta_given_y, alpha=0.7)\n",
    "\n",
    "    plt.axvline(theta_real, ymax=0.3, color='k')\n",
    "    plt.plot(0, 0, label=f'{N:4d} trials\\n{y:4d} heads', alpha=0)\n",
    "    plt.xlim(0, 1)\n",
    "    plt.ylim(0, 12)\n",
    "    plt.legend()\n",
    "    plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig('B11197_01_05.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### One will find that as more trial samples, the posterior distributions of different priod distribution converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data generation Process by y = 4 + 2*x + random noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,10,500) \n",
    "y = 4 + 2*x + np.random.standard_normal(len(x))*2\n",
    "# Linear regression\n",
    "reg = np.polyfit(x,y,1)\n",
    "print(f\"sample size = \",len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Graph of Classical Rgression Based on Frequentist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,4)) \n",
    "plt.scatter(x,y,c = y, marker= 'v') \n",
    "plt.plot(x,reg [1] + reg [0]*x, lw = 2.0) \n",
    "plt.colorbar() \n",
    "plt.grid (True) \n",
    "plt.xlabel('x') \n",
    "plt.ylabel('y') \n",
    "# tag: pm_fig_0\n",
    "# title: Sample data points a regresion line\n",
    "# size: 90"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regression Coefficients of the Classical Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model: \n",
    "        # model specifications in PyMC3\n",
    "        # are wrapped in a with-statement\n",
    "    # define priors\n",
    "    alpha = pm.Normal('alpha', mu=0, sd=20)\n",
    "    beta = pm.Normal('beta', mu=0, sd=20)\n",
    "    sigma = pm.Uniform('sigma', lower=0, upper=10)\n",
    "    \n",
    "    # define linear regression\n",
    "    y_est = alpha + beta * x\n",
    "    \n",
    "    # define likelihood\n",
    "    likelihood = pm.Normal('y', mu=y_est, sd=sigma, observed=y)\n",
    "    \n",
    "    # inference\n",
    "    start = pm.find_MAP()\n",
    "      # find starting value by optimization\n",
    "    step = pm.NUTS()\n",
    "      # instantiate MCMC sampling algorithm\n",
    "    trace = pm.sample(100, step, start=start, progressbar=True)\n",
    "      # draw 100 posterior samples using NUTS sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = pm.plots.traceplot(trace)\n",
    "plt.figure(figsize=(8, 8))\n",
    "#tag: pm_fig_1\n",
    "#title: Trace plots for alpha, beta and sigma\n",
    "# size: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Bayesian regression result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.scatter(x, y, c=y, marker='v')\n",
    "plt.colorbar()\n",
    "plt.grid(True)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "for i in range(len(trace)):\n",
    "    plt.plot(x, trace['alpha'][i] + trace['beta'][i] * x)\n",
    "# tag: pm_fig_2\n",
    "# title: Sample data and regression lines from Bayesian regression\n",
    "# size: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
